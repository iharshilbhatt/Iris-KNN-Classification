# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvaG0EQ1HLGVsj24Zlp3LHm0Qc46MGuh
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

# Assign colum names to the dataset
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

# Read dataset to pandas dataframe
dataset = pd.read_csv(url, names=names)

dataset.head()

dataset

num_neighbors = 5

training=dataset.values[0:148,0:4]
training

def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column])

for i in range(len(training[0])):
  str_column_to_float(training,i)

training

trainingclass=dataset.values[0:148,-1]
trainingclass

def euclidean_distance(row1, row2):
	distance = 0.0
	for i in range(len(row1)-1):
		distance += (row1[i] - row2[i])**2
	return (math.sqrt(distance))

testing=dataset.values[148,0:4]
testing

unique_list=[]
for x in trainingclass:
  if x not in unique_list:
    unique_list.append(x)

print(unique_list)
for i in range(len(trainingclass)):
  for k in range(len(unique_list)):
    if(trainingclass[i]==unique_list[k]):
      trainingclass[i]=k
print(trainingclass)

distance=[]
for i in range(len(training)):
  distance.append([euclidean_distance((training[i]),(testing)),trainingclass[i]])

distance

distance.sort()
distance

identify=[0 for i in range(len(unique_list))]
for i in range(num_neighbors):
  identify[distance[i][1]]=identify[distance[i][1]]+1

identify

print("The predicted class is: ",unique_list[identify.index(max(identify))])

# Set a seed for reproducibility
np.random.seed(0)

# Shuffle the dataset
shuffled_dataset = dataset.sample(frac=1)

# Split into training and testing datasets (80% train, 20% test)
train_size = int(0.8 * len(shuffled_dataset))
train_set = shuffled_dataset.iloc[:train_size]
test_set = shuffled_dataset.iloc[train_size:]

# Separate features and labels for both sets
X_train = train_set.iloc[:, :-1].values
y_train = train_set.iloc[:, -1].values
X_test = test_set.iloc[:, :-1].values
y_test = test_set.iloc[:, -1].values

# Convert string labels to numerical labels
unique_labels = np.unique(y_train)
label_mapping = {label: i for i, label in enumerate(unique_labels)}
y_train_numeric = np.array([label_mapping[label] for label in y_train])
y_test_numeric = np.array([label_mapping[label] for label in y_test])

# Convert string features to float
def str_column_to_float(dataset, column):
    for row in dataset:
        row[column] = float(row[column])

for i in range(X_train.shape[1]):
    str_column_to_float(X_train, i)
    str_column_to_float(X_test, i)

# Now you have your randomized and split datasets:
# X_train, y_train_numeric for training
# X_test, y_test_numeric for testing

# You can use these to train your k-NN model and generate predictions for the test set.

# Import necessary library for k-NN
from sklearn.neighbors import KNeighborsClassifier

# Initialize the k-NN model
k = 3  # Set the number of neighbors
model = KNeighborsClassifier(n_neighbors=k)

# Train the k-NN model
model.fit(X_train, y_train_numeric)

# Generate predictions for the test set
y_pred = model.predict(X_test) # Calculate y_pred here

# For confusion matrix and accuracy, you can use libraries like scikit-learn:
from sklearn.metrics import confusion_matrix, accuracy_score


# Confusion Matrix
cm = confusion_matrix(y_test_numeric, y_pred)
print("Confusion Matrix:\n", cm)
# Accuracy
accuracy = accuracy_score(y_test_numeric, y_pred)
print("Accuracy:", accuracy)